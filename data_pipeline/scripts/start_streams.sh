#!/bin/bash
# start_streams.sh

echo "ðŸš€ Starting Spark Streaming Jobs..."

# 1. Job Real-time Analytics (Ghi MongoDB) - Cháº¡y ngáº§m (-d)
docker exec -d spark-master spark-submit \
  --master spark://spark-master:7077 \
  --driver-memory 512m \
  --executor-memory 800m \
  --total-executor-cores 1 \
  --conf "spark.kafka.consumer.cache.enabled=false" \
  /opt/src/jobs/stream_to_mongo.py

echo "âœ… Job MongoDB Started."

# 2. Job Archiver (Ghi MinIO) - Cháº¡y ngáº§m (-d)
docker exec -d spark-master spark-submit \
  --master spark://spark-master:7077 \
  --driver-memory 512m \
  --executor-memory 800m \
  --total-executor-cores 1 \
  --conf "spark.kafka.consumer.cache.enabled=false" \
  /opt/src/jobs/stream_to_minio.py

echo "âœ… Job MinIO Started."