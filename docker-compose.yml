services:
  # -----------------------------------------------------------
  # 1. KAFKA (Apache Official - KRaft Mode)
  # -----------------------------------------------------------
  kafka:
    image: apache/kafka:3.7.0
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092" # Port nội bộ cố định
      - "${KAFKA_PORT_EXTERNAL}:9094" # Port ngoài lấy từ .env
    env_file:
      - .env
    environment:
      # KRaft Mode Configuration
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093

      # Listener Configuration
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,EXTERNAL://localhost:${KAFKA_PORT_EXTERNAL}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

      # Cluster Configuration
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: ${KAFKA_NUM_PARTITIONS}
      KAFKA_DEFAULT_REPLICATION_FACTOR: ${KAFKA_REPLICATION_FACTOR}

      KAFKA_MESSAGE_MAX_BYTES: 10485760
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
      KAFKA_LOG_DIRS: /var/lib/kafka/data

      KAFKA_HEAP_OPTS: "-Xmx400M -Xms400M"
    networks:
      - bigdata-net
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: [ "CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list || exit 1" ]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - bigdata-net

  # -----------------------------------------------------------
  # 2. SPARK MASTER
  # -----------------------------------------------------------
  spark-master:
    build:
      context: .
      dockerfile: data_pipeline/spark.Dockerfile
    image: my-apache-spark:3.5
    container_name: spark-master
    hostname: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:9090"
      - "7077:7077"
      - "4040:4040"
    env_file:
      - .env
    environment:
      - SPARK_MASTER_WEBUI_PORT=9090
      - SPARK_CONF_DIR=/opt/spark/conf
      - PYTHONPATH=/opt/src
      - HF_HOME=/tmp/huggingface_cache
      - TRANSFORMERS_CACHE=/tmp/huggingface_cache
      - SENTENCE_TRANSFORMERS_HOME=/tmp/huggingface_cache
      - HUGGINGFACE_HUB_CACHE=/tmp/huggingface_cache
      - XET_LOG_DIR=/tmp/huggingface_cache/xet_logs
    volumes:
      - ./data:/opt/data
      - ./data_pipeline:/opt/src
      - ./common:/opt/src/common
      - ./huggingface_cache:/tmp/huggingface_cache
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - bigdata-net

  # -----------------------------------------------------------
  # 3. SPARK WORKER
  # -----------------------------------------------------------
  spark-worker:
    image: my-apache-spark:3.5
    # container_name: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - PYTHONPATH=/opt/src
    env_file:
      - .env
    depends_on:
      - spark-master
    volumes:
      - ./data:/opt/data
      - ./data_pipeline:/opt/src
      - ./common:/opt/src/common
    networks:
      - bigdata-net
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure

  # -----------------------------------------------------------
  # 4. MINIO (Data Lake)
  # -----------------------------------------------------------
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ACCESS_KEY}
      - MINIO_ROOT_PASSWORD=${MINIO_SECRET_KEY}
    env_file:
      - .env
    networks:
      - bigdata-net
    volumes:
      - minio_data:/data

  # -----------------------------------------------------------
  # 5. MONGODB (Database)
  # -----------------------------------------------------------
  mongodb:
    image: mongo:latest
    container_name: mongodb
    env_file:
      - .env
    environment:
      - MONGO_INITDB_ROOT_USERNAME=${MONGO_INITDB_ROOT_USERNAME}
      - MONGO_INITDB_ROOT_PASSWORD=${MONGO_INITDB_ROOT_PASSWORD}
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    networks:
      - bigdata-net

  # -----------------------------------------------------------
  # 6. JOBS & APPS (Đổi tên & Thêm mới)
  # -----------------------------------------------------------
  # Service đồng bộ Master Data vào MongoDB hàng ngày
  job-sync-master:
    image: my-apache-spark:3.5
    container_name: job-sync-master
    user: root
    depends_on:
      - mongodb
      - spark-master
    env_file:
      - .env
    environment:
      - PYTHONPATH=/opt/src
    volumes:
      - ./data_pipeline:/opt/src
      - ./data:/opt/data
      - ./common:/opt/src/common
    profiles: ["tools"]
    command: >
      /bin/bash -c "
        echo 'Starting Master Data Sync Job...';
        python3 -u /opt/src/batch/import_songs_master_data.py;
        python3 -u /opt/src/batch/import_users_master_data.py;
        echo 'Sync Success! Sleeping for 24 hours...';
        "
    networks:
      - bigdata-net

  # [APP] Service mới cho Web/API
  backend:
    build:
      context: .
      dockerfile: /backend/api.Dockerfile
    container_name: music-backend
    ports:
      - "${WEB_APP_PORT}:8000"
    env_file:
      - .env
    environment:
      - PYTHONPATH=/app
    depends_on:
      - mongodb
      - redis
      - milvus
      - etcd
      - kafka
    volumes:
      - ./backend:/app
      - ./common:/app/common
      - ./data:/app/data
    command: python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    networks:
      - bigdata-net

  frontend:
    build: 
      context: ./frontend
      dockerfile: frontend.Dockerfile
    container_name: music-frontend
    env_file:
      - .env
    ports:
      - "${FRONTEND_PORT}:5173"
    volumes:
      - ./frontend:/app             
      - /app/node_modules
    environment:
      - CHOKIDAR_USEPOLLING=true    # Hỗ trợ Hot Reload
    networks:
      - bigdata-net

  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    container_name: milvus-etcd
    environment:
      - ETCD_AUTO_COMPACTION_RETENTION=1
      - ETCD_LOG_LEVEL=warn
      - ETCD_ADVERTISE_CLIENT_URLS=http://etcd:2379
      - ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
      - ETCD_DATA_DIR=/etcd-data
    volumes:
      - etcd_data:/etcd-data
    networks:
      - bigdata-net
    healthcheck:
      test: [ "CMD", "etcdctl", "endpoint", "health" ]
      interval: 10s
      timeout: 5s
      retries: 5

  milvus:
    image: milvusdb/milvus:v2.3.15
    container_name: milvus-standalone
    ports:
      - "19530:19530"
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD}
      - MINIO_BUCKET_NAME=a-bucket
    env_file:
      - .env
    depends_on:
      - minio
      - etcd
    command: [ "milvus", "run", "standalone" ]
    volumes:
      - milvus_data:/var/lib/milvus
    networks:
      - bigdata-net
    restart: always

  redis:
    image: redis:7.2-alpine
    container_name: music-recsys-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    networks:
      - bigdata-net

  orchestrator:
    build:
      context: .
      dockerfile: data_pipeline/orchestrator.Dockerfile
    container_name: pipeline-orchestrator
    depends_on:
      - spark-master
      - kafka
      - backend
      - minio
      - mongodb
    env_file:
      - .env
    environment:
      - PYTHONPATH=/opt/src
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock # Bắt buộc
      - ./data_pipeline:/opt/src
      - ./common:/opt/src/common
    restart: always
    networks:   
      - bigdata-net

networks:
  bigdata-net:
    name: ${NETWORK_NAME}
    driver: bridge

volumes:
  mongo_data:
  kafka_data:
  minio_data:
  etcd_data:
  milvus_data:
  redis_data:
