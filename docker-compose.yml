services:
  # -----------------------------------------------------------
  # 1. KAFKA (Apache Official - KRaft Mode)
  # -----------------------------------------------------------
  kafka:
    image: apache/kafka:3.7.0
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092" # Port ná»™i bá»™ cá»‘ Ä‘á»‹nh
      - "${KAFKA_PORT_EXTERNAL}:9094" # Port ngoÃ i láº¥y tá»« .env
    env_file:
      - .env
    environment:
      # KRaft Mode Configuration
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093

      # Listener Configuration
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,EXTERNAL://localhost:${KAFKA_PORT_EXTERNAL}
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT

      # Cluster Configuration
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_NUM_PARTITIONS: ${KAFKA_NUM_PARTITIONS}
      KAFKA_DEFAULT_REPLICATION_FACTOR: ${KAFKA_REPLICATION_FACTOR}

      KAFKA_MESSAGE_MAX_BYTES: 10485760
      KAFKA_REPLICA_FETCH_MAX_BYTES: 10485760
      CLUSTER_ID: 'MkU3OEVBNTcwNTJENDM2Qk'
      KAFKA_LOG_DIRS: /var/lib/kafka/data
    networks:
      - bigdata-net
    volumes:
      - kafka_data:/var/lib/kafka/data
    healthcheck:
      test: [ "CMD-SHELL", "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list || exit 1" ]
      interval: 15s
      timeout: 10s
      retries: 10
      start_period: 60s

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - bigdata-net

  # -----------------------------------------------------------
  # 2. SPARK MASTER
  # -----------------------------------------------------------
  spark-master:
    build:
      context: .
      dockerfile: data_pipeline/spark.Dockerfile
    image: my-apache-spark:3.5
    container_name: spark-master
    hostname: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "${SPARK_MASTER_WEBUI_PORT}:8080"
      - "7077:7077"
    env_file:
      - .env
    environment:
      - SPARK_CONF_DIR=/opt/spark/conf
      - PYTHONPATH=/opt/src
    volumes:
      - ./data:/opt/data
      - ./data_pipeline:/opt/src
      - ./common:/opt/src/common
    depends_on:
      kafka:
        condition: service_healthy
    networks:
      - bigdata-net

  # -----------------------------------------------------------
  # 3. SPARK WORKER
  # -----------------------------------------------------------
  spark-worker:
    image: my-apache-spark:3.5
    # container_name: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - PYTHONPATH=/opt/src
    env_file:
      - .env
    depends_on:
      - spark-master
    volumes:
      - ./data:/opt/data
      - ./data_pipeline:/opt/src
      - ./common:/opt/src/common
    networks:
      - bigdata-net
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure

  # -----------------------------------------------------------
  # 4. MINIO (Data Lake)
  # -----------------------------------------------------------
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "${MINIO_CONSOLE_PORT}:9001"
    command: server /data --console-address ":9001"
    env_file:
      - .env
    networks:
      - bigdata-net
    volumes:
      - minio_data:/data

  # -----------------------------------------------------------
  # 5. MONGODB (Database)
  # -----------------------------------------------------------
  mongodb:
    image: mongo:latest
    container_name: mongodb
    ports:
      - "27017:27017"
    volumes:
      - mongo_data:/data/db
    networks:
      - bigdata-net
  
  # -----------------------------------------------------------
  # 6. JOBS & APPS (Äá»•i tÃªn & ThÃªm má»›i)
  # -----------------------------------------------------------
  # [JOB] Service Ä‘á»“ng bá»™ Master Data vaÌ€o MongoDB hÃ ng ngÃ y
  job-sync-master:
    image: my-apache-spark:3.5
    container_name: job-sync-master
    user: root 
    depends_on:
      - mongodb
      - spark-master
    env_file:
      - .env
    environment:
      - PYTHONPATH=/opt/src
    volumes:
      - ./data_pipeline:/opt/src
      - ./data:/opt/data
      - ./common:/opt/src/common
    command: >
      /bin/bash -c "
        echo 'ðŸš€ Starting Master Data Sync Job...';
        python3 -u /opt/src/batch/import_master_songs.py;
        echo 'âœ… Sync Success! Sleeping for 24 hours...';
        "
    networks:
      - bigdata-net

  job-producer:
    image: my-apache-spark:3.5
    container_name: music_log_producer
    depends_on:
      - kafka
    env_file:
      - .env
    environment:
      - PYTHONPATH=/opt/src
    volumes:
      - ./data_pipeline:/opt/src
      - ./data:/opt/data
      - ./common:/opt/src/common
    # command: ["python3", "/opt/src/ingestion/producer.py"] 
    networks:
      - bigdata-net
    restart: on-failure

  job-consumer:
    image: my-apache-spark:3.5
    container_name: music_spark_streaming
    depends_on:
      - kafka
      - spark-master
      - minio
    env_file:
      - .env
    environment:
      - PYTHONPATH=/opt/src
      - SPARK_MASTER_URL=spark://spark-master:7077
    volumes:
      - ./data_pipeline:/opt/src
      - ./data:/opt/data
      - ./common:/opt/src/common
    # command: >
    #   /opt/spark/bin/spark-submit 
    #   --master spark://spark-master:7077
    #   /opt/src/ingestion/kafka_to_minio.py
    networks:
      - bigdata-net
    restart: on-failure



  # [APP] Service má»›i cho Web/API
  backend:
    build:
      context: .
      dockerfile: backend/api.Dockerfile
    container_name: music_backend
    ports:
      - "8000:8000"
    environment:
      - MONGO_URI=mongodb://mongodb:27017
      - REDIS_HOST=redis
      - MILVUS_HOST=milvus
      - PYTHONPATH=/app
    depends_on:
      - mongodb
      - redis
      - milvus
    volumes:
      - ./backend/app:/app/app
      - ./common:/app/common
    command: python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    networks:
      - bigdata-net

  etcd:
    image: quay.io/coreos/etcd:v3.5.5
    container_name: milvus-etcd
    environment:
      - ETCD_AUTO_COMPACTION_RETENTION=1
      - ETCD_LOG_LEVEL=warn
      - ETCD_ADVERTISE_CLIENT_URLS=http://etcd:2379
      - ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
      - ETCD_DATA_DIR=/etcd-data
    volumes:
      - etcd_data:/etcd-data
    networks:
      - bigdata-net
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 10s
      timeout: 5s
      retries: 5

  milvus:
    image: milvusdb/milvus:v2.3.15
    container_name: milvus-standalone
    ports:
      - "19530:19530"
    environment:
      - ETCD_ENDPOINTS=etcd:2379
      - MINIO_ADDRESS=minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD}
    env_file:
      - .env
    depends_on:
      - minio
      - etcd
    command: ["milvus", "run", "standalone"]
    volumes:
      - milvus_data:/var/lib/milvus
    networks:
      - bigdata-net

  redis:
    image: redis:7.2-alpine
    container_name: music_recsys_redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    networks:
      - bigdata-net

networks:
  bigdata-net:
    name: ${NETWORK_NAME}
    driver: bridge

volumes:
  mongo_data:
  kafka_data:
  minio_data:
  etcd_data:
  milvus_data:
  redis_data:
