---
description: Cháº¡y pipeline streaming tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i (Download â†’ Clean â†’ Sort â†’ Kafka â†’ Spark â†’ MinIO â†’ MongoDB)
---
# Music Recommendation System - Full Pipeline

Pipeline xá»­ lÃ½ data streaming tá»« HuggingFace Ä‘áº¿n MinIO Data Lake vÃ  MongoDB.

## Kiáº¿n trÃºc Pipeline

```
HuggingFace Dataset (LastFM-1K)
        â†“
   [Download]
        â†“
  data/raw/*.parquet
        â†“
   [Clean/Fix Format]
        â†“
  data/data_clean/*.parquet
        â†“
   [Sort by Timestamp]
        â†“
  data/processed_sorted/*.parquet
        â†“
   [Producer] â”€â”€â†’ Kafka Topic "music_log"
                         â†“
               [Spark Streaming ETL]
                         â†“
               MinIO (s3a://datalake/raw/music_logs/)
                         â†“
               [ETL Master Data]
                         â†“
               MongoDB (music_recsys.songs)
```

---

## BÆ¯á»šC 0: Khá»Ÿi Ä‘á»™ng Infrastructure

```bash
# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker-compose up -d

# Kiá»ƒm tra services Ä‘Ã£ sáºµn sÃ ng
docker-compose ps
```

Äá»£i Ä‘áº¿n khi táº¥t cáº£ services healthy (khoáº£ng 1-2 phÃºt).

**CÃ¡c services cáº§n cháº¡y:**

- `kafka` (Healthy)
- `kafka-ui`
- `spark-master`
- `spark-worker`
- `minio`
- `mongodb`

---

## BÆ¯á»šC 1: Download Data tá»« HuggingFace

// turbo

```bash
docker exec -it spark-master python3 /opt/src/prepare-data/download_data.py
```

**Output mong Ä‘á»£i:**

```
Äang download dataset tá»« HuggingFace: matthewfranglen/lastfm-1k...
Xá»­ lÃ½ split: train
  - Sá»‘ dÃ²ng: xxx
  - ÄÃ£ lÆ°u: /opt/data/raw/train.parquet
...
HOÃ€N THÃ€NH!
```

**Kiá»ƒm tra:**

```bash
docker exec spark-master ls -lh /opt/data/raw/
```

---

## ğŸ§¹ BÆ¯á»šC 2: Clean Data (Fix Format)

// turbo

```bash
docker exec -it spark-master python3 /opt/src/prepare-data/fix_format.py
```

**Output mong Ä‘á»£i:**

```
 Báº®T Äáº¦U QUY TRÃŒNH TÃI SINH Dá»® LIá»†U
 Xá»­ lÃ½: train.parquet
   ÄÃ£ tÃ¡i sinh thÃ nh cÃ´ng
...
```

**Kiá»ƒm tra:**

```bash
docker exec spark-master ls -lh /opt/data/data_clean/
```

---

## BÆ¯á»šC 3: Sort Data theo Timestamp

// turbo

```bash
docker exec -it spark-master spark-submit /opt/src/prepare-data/etl_sort.py
```

**Output mong Ä‘á»£i:**

```
Khá»Ÿi Ä‘á»™ng Spark Session...
Äang Ä‘á»c dá»¯ liá»‡u sáº¡ch tá»«: /opt/data/data_clean/*.parquet
Äang sáº¯p xáº¿p theo thá»i gian...
Äang ghi dá»¯ liá»‡u Ä‘Ã£ sáº¯p xáº¿p ra: /opt/data/processed_sorted
THÃ€NH CÃ”NG!
```

**Kiá»ƒm tra:**

```bash
docker exec spark-master ls -lh /opt/data/processed_sorted/
```

---

## BÆ¯á»šC 4: Khá»Ÿi Ä‘á»™ng Spark Streaming ETL (Terminal 1)

**Má»Ÿ Terminal má»›i** vÃ  cháº¡y:

```bash
docker exec -it spark-master bash -c "cd /opt/src/ingestion && spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 stream_to_minio.py"
```

**Output mong Ä‘á»£i:**

```
Khá»Ÿi Ä‘á»™ng Spark Streaming ETL...
Äang láº¯ng nghe Kafka Topic 'music_log'...
Äang ghi xuá»‘ng MinIO (Parquet)...
```

**QUAN TRá»ŒNG: Giá»¯ terminal nÃ y má»Ÿ, Ä‘á»«ng táº¯t!**

---

## BÆ¯á»šC 5: Cháº¡y Producer (Terminal 2)

**Má»Ÿ Terminal má»›i** vÃ  cháº¡y:

```bash
docker exec -it spark-master python3 /opt/src/ingestion/producer.py
```

**Output mong Ä‘á»£i:**

```
Äang kiá»ƒm tra Topic 'music_log'...
Topic 'music_log' Ä‘Ã£ tá»“n táº¡i.
Khá»Ÿi táº¡o Producer...
Báº¯t Ä‘áº§u Replay vá»›i tá»‘c Ä‘á»™: x200.0
Äá»c file: ...
Sent: 100 | Time: ...
Sent: 200 | Time: ...
```

 **Äá»£i khoáº£ng 1-2 phÃºt** Ä‘á»ƒ cÃ³ Ä‘á»§ data ghi vÃ o MinIO, sau Ä‘Ã³ nháº¥n `Ctrl+C` Ä‘á»ƒ dá»«ng Producer.

---

## BÆ¯á»šC 6: Kiá»ƒm tra Káº¿t quáº£ trong MinIO

```bash
# Kiá»ƒm tra data Ä‘Ã£ Ä‘Æ°á»£c ghi
docker exec minio mc ls local/datalake/raw/music_logs/ --recursive --summarize
```

**Hoáº·c truy cáº­p MinIO Console:**

- URL: http://localhost:9001
- Username: `minioadmin`
- Password: `minioadmin`
- Navigate to: `datalake` â†’ `raw` â†’ `music_logs`

---

## BÆ¯á»šC 7: ETL Master Data (MinIO â†’ MongoDB)

**Dá»«ng Spark Streaming** (Terminal 1) báº±ng `Ctrl+C`, sau Ä‘Ã³ restart Spark cluster:

```bash
# Restart Spark Ä‘á»ƒ giáº£i phÃ³ng resources
docker restart spark-master spark-worker

# Äá»£i 15-20 giÃ¢y cho cluster sáºµn sÃ ng
```

**Cháº¡y ETL Master Data:**

```bash
docker exec spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,org.mongodb.spark:mongo-spark-connector_2.12:10.3.0 /opt/src/processing/etl_master_data.py
```

**Output mong Ä‘á»£i:**

```
Báº¯t Ä‘áº§u ETL Master Data (Collection: songs)...
>>> Äang Ä‘á»c dá»¯ liá»‡u tá»« MinIO...
>>> Äang lá»c bÃ i hÃ¡t duy nháº¥t...
>>> Äang ghi vÃ o MongoDB...
THÃ€NH CÃ”NG! ÄÃ£ lÆ°u xxx bÃ i hÃ¡t vÃ o MongoDB.
```

---

## BÆ¯á»šC 8: Kiá»ƒm tra MongoDB

```bash
# Äáº¿m sá»‘ bÃ i hÃ¡t
docker exec mongodb mongo music_recsys --eval "db.songs.count()"

# Xem 5 bÃ i hÃ¡t Ä‘áº§u tiÃªn
docker exec mongodb mongo music_recsys --eval "db.songs.find().limit(5).pretty()"
```

---

## Monitoring & Debug

### Kafka UI

- URL: http://localhost:8080
- Xem topic `music_log` vÃ  sá»‘ messages

### Spark Master UI

- URL: http://localhost:9090
- Xem running applications vÃ  jobs

### MinIO Console

- URL: http://localhost:9001
- Xem data Ä‘Ã£ Ä‘Æ°á»£c ghi

---

## Dá»«ng Pipeline

```bash
# Terminal cháº¡y Producer: Ctrl+C
# Terminal cháº¡y Spark Streaming: Ctrl+C

# Táº¯t táº¥t cáº£ services
docker-compose down
```

---

## Cháº¡y láº¡i tá»« Ä‘áº§u (Reset)

```bash
# 1. XÃ³a data cÅ© trÃªn mÃ¡y local
rm -rf data/raw data/data_clean data/processed_sorted

# 2. XÃ³a MinIO data (trong container)
docker exec minio mc rm local/datalake/checkpoints/ --recursive --force
docker exec minio mc rm local/datalake/raw/ --recursive --force

# 3. XÃ³a MongoDB data
docker exec mongodb mongo music_recsys --eval "db.songs.drop()"

# 4. Cháº¡y láº¡i tá»« BÆ¯á»šC 1
```

---

## Cáº¥u hÃ¬nh quan trá»ng

| Config             | File                                           | GiÃ¡ trá»‹                     |
| ------------------ | ---------------------------------------------- | ----------------------------- |
| Kafka Bootstrap    | `producer.py`, `stream_to_minio.py`        | `kafka:9092`                |
| MinIO Endpoint     | `stream_to_minio.py`, `etl_master_data.py` | `http://minio:9000`         |
| MongoDB URI        | `etl_master_data.py`                         | `mongodb://mongodb:27017`   |
| Spark Master       | `etl_master_data.py`                         | `spark://spark-master:7077` |
| Processing Trigger | `stream_to_minio.py`                         | `1 minute`                  |
| Producer Speed     | `producer.py`                                | `x200`                      |

---

## Troubleshooting

### 1. Spark job bÃ¡o "Initial job has not accepted any resources"

**NguyÃªn nhÃ¢n:** Spark Worker chÆ°a register hoáº·c Ä‘ang báº­n vá»›i job khÃ¡c.

**Giáº£i phÃ¡p:**

```bash
# Restart Spark cluster
docker restart spark-master spark-worker

# Äá»£i 15-20 giÃ¢y rá»“i cháº¡y láº¡i
```

### 2. MongoDB khÃ´ng khá»Ÿi Ä‘á»™ng Ä‘Æ°á»£c (Exit code 62)

**NguyÃªn nhÃ¢n:** PhiÃªn báº£n MongoDB má»›i yÃªu cáº§u CPU há»— trá»£ AVX.

**Giáº£i phÃ¡p:** Sá»­ dá»¥ng MongoDB 4.4 trong file `.env`:

```
MONGO_IMAGE=mongo:4.4
```

### 3. MinIO Console khÃ´ng truy cáº­p Ä‘Æ°á»£c

**NguyÃªn nhÃ¢n:** Docker Desktop chÆ°a cháº¡y hoáº·c container chÆ°a start.

**Giáº£i phÃ¡p:**

```bash
# Kiá»ƒm tra Docker Ä‘ang cháº¡y
docker ps

# Khá»Ÿi Ä‘á»™ng láº¡i services
docker-compose up -d
```

### 4. Producer khÃ´ng gá»­i Ä‘Æ°á»£c message

**NguyÃªn nhÃ¢n:** Kafka chÆ°a healthy.

**Giáº£i phÃ¡p:**

```bash
# Kiá»ƒm tra Kafka status
docker-compose ps kafka

# Äá»£i Ä‘áº¿n khi Kafka healthy rá»“i cháº¡y láº¡i
```

---

## ğŸ“ Cáº¥u trÃºc Project

```
music-recsys/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Data download tá»« HuggingFace
â”‚   â”œâ”€â”€ data_clean/             # Data Ä‘Ã£ clean
â”‚   â””â”€â”€ processed_sorted/       # Data Ä‘Ã£ sort theo timestamp
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ prepare-data/
â”‚   â”‚   â”œâ”€â”€ download_data.py    # Download tá»« HuggingFace
â”‚   â”‚   â”œâ”€â”€ fix_format.py       # Clean data
â”‚   â”‚   â””â”€â”€ etl_sort.py         # Sort theo timestamp
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ producer.py         # Gá»­i data vÃ o Kafka
â”‚   â”‚   â””â”€â”€ stream_to_minio.py  # Spark Streaming: Kafka â†’ MinIO
â”‚   â””â”€â”€ processing/
â”‚       â””â”€â”€ etl_master_data.py  # ETL: MinIO â†’ MongoDB
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â””â”€â”€ run-pipeline.md             # File hÆ°á»›ng dáº«n nÃ y
```
