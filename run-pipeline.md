---
description: Cháº¡y pipeline streaming tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i (Download â†’ Clean â†’ Sort â†’ Kafka â†’ Spark â†’ MinIO â†’ MongoDB â†’ Milvus)
---
# Music Recommendation System - Full Pipeline

Pipeline xá»­ lÃ½ data streaming tá»« HuggingFace Ä‘áº¿n MinIO Data Lake, MongoDB vÃ  Milvus.

## Kiáº¿n trÃºc Pipeline

```
HuggingFace Dataset (LastFM-1K)
        â†“
   [Download]
        â†“
  data/raw/*.parquet
        â†“
   [Clean/Fix Format]
        â†“
  data/data_clean/*.parquet
        â†“
   [Sort by Timestamp]
        â†“
  data/processed_sorted/*.parquet
        â†“
   [Producer TURBO] â”€â”€â†’ Kafka Topic "music_log"
                             â†“
               [Spark Streaming TURBO ETL]
                             â†“
               MinIO (s3a://datalake/raw/music_logs/)
                             â†“
               [ETL Master Data]
                             â†“
               MongoDB (music_recsys.songs)
                             â†“
               [ETL Users]
                             â†“
               MongoDB (music_recsys.users)
                             â†“
               [Train ALS Model]
                    â†“         â†“
           MongoDB        Milvus
        (user vectors)  (item vectors)
```

---

## BÆ¯á»šC 0: Khá»Ÿi Ä‘á»™ng Infrastructure

```bash
# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker-compose up -d

# Kiá»ƒm tra services Ä‘Ã£ sáºµn sÃ ng
docker-compose ps
```

Äá»£i Ä‘áº¿n khi táº¥t cáº£ services healthy (khoáº£ng 2-3 phÃºt cho Milvus).

**CÃ¡c services cáº§n cháº¡y:**

- `kafka` (Healthy)
- `kafka-ui`
- `spark-master`
- `spark-worker`
- `minio`
- `mongodb`
- `milvus-etcd`
- `milvus-standalone`

**Kiá»ƒm tra Milvus Ä‘Ã£ sáºµn sÃ ng:**

```bash
docker logs milvus-standalone 2>&1 | tail -5
# Náº¿u tháº¥y "Milvus Proxy successfully initialized" lÃ  OK
```

---

## BÆ¯á»šC 1: Download Data tá»« HuggingFace

// turbo

```bash
docker exec -it spark-master python3 /opt/src/prepare-data/download_data.py
```

**Output mong Ä‘á»£i:**

```
Äang download dataset tá»« HuggingFace: matthewfranglen/lastfm-1k...
Xá»­ lÃ½ split: train
  - Sá»‘ dÃ²ng: xxx
  - ÄÃ£ lÆ°u: /opt/data/raw/train.parquet
...
HOÃ€N THÃ€NH!
```

**Kiá»ƒm tra:**

```bash
docker exec spark-master ls -lh /opt/data/raw/
```

---

## ğŸ§¹ BÆ¯á»šC 2: Clean Data (Fix Format)

// turbo

```bash
docker exec -it spark-master python3 /opt/src/prepare-data/fix_format.py
```

**Output mong Ä‘á»£i:**

```
 Báº®T Äáº¦U QUY TRÃŒNH TÃI SINH Dá»® LIá»†U
 Xá»­ lÃ½: train.parquet
   ÄÃ£ tÃ¡i sinh thÃ nh cÃ´ng
...
```

**Kiá»ƒm tra:**

```bash
docker exec spark-master ls -lh /opt/data/data_clean/
```

---

## BÆ¯á»šC 3: Sort Data theo Timestamp

// turbo

```bash
docker exec -it spark-master spark-submit /opt/src/prepare-data/etl_sort.py
```

**Output mong Ä‘á»£i:**

```
Khá»Ÿi Ä‘á»™ng Spark Session...
Äang Ä‘á»c dá»¯ liá»‡u sáº¡ch tá»«: /opt/data/data_clean/*.parquet
Äang sáº¯p xáº¿p theo thá»i gian...
Äang ghi dá»¯ liá»‡u Ä‘Ã£ sáº¯p xáº¿p ra: /opt/data/processed_sorted
THÃ€NH CÃ”NG!
```

**Kiá»ƒm tra:**

```bash
docker exec spark-master ls -lh /opt/data/processed_sorted/
```

---

## BÆ¯á»šC 4: Khá»Ÿi Ä‘á»™ng Spark Streaming TURBO ETL (Terminal 1)

**Má»Ÿ Terminal má»›i** vÃ  cháº¡y:

```bash
docker exec -it spark-master bash -c "cd /opt/src/pipelines/ingestion && spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 stream_to_minio_turbo.py"
```

> âš¡ **TURBO Mode**: Trigger má»—i 10 giÃ¢y (thay vÃ¬ 1 phÃºt), fetch size lá»›n hÆ¡n, tá»‘i Æ°u S3A upload.

**Output mong Ä‘á»£i:**

```
ğŸš€ Khá»Ÿi Ä‘á»™ng Spark Streaming TURBO ETL...
âš¡ Äang láº¯ng nghe Kafka Topic 'music_log' vá»›i TURBO settings...
ğŸ’¾ Äang ghi xuá»‘ng MinIO (Parquet) - TURBO MODE (10s trigger)...
```

**QUAN TRá»ŒNG: Giá»¯ terminal nÃ y má»Ÿ, Ä‘á»«ng táº¯t!**

---

## BÆ¯á»šC 5: Cháº¡y Producer (Terminal 2)

**Má»Ÿ Terminal má»›i** vÃ  chá»n 1 trong 3 mode:

### ğŸ¯ Option A: BALANCED Mode (Khuyáº¿n nghá»‹)

```bash
docker exec -it spark-master python3 /opt/src/pipelines/ingestion/producer_balanced.py
```

> âš–ï¸ **BALANCED Mode**: CÃ¢n báº±ng giá»¯a tá»‘c Ä‘á»™ vÃ  tÃ­nh realtime
> - Giá»¯ timestamp gá»‘c (quan trá»ng cho time-series analytics)
> - TÄƒng tá»‘c x1000 (1 giá» data = 3.6 giÃ¢y thá»±c)
> - Nháº£y qua khoáº£ng trá»‘ng > 5 phÃºt
> - Tá»‘c Ä‘á»™: ~1,000-5,000 msg/s

**Output mong Ä‘á»£i:**

```
âš¡ BALANCED MODE: x1000.0 speed, batch 500
   Skip gaps > 5 minutes
ğŸ“– Äá»c file: part-00000-xxx.parquet
ğŸ“Š Sent: 5,000 | Rate: 2,345 msg/s | Elapsed: 2.1s
â© Skip gap 45.2 phÃºt
ğŸ“Š Sent: 10,000 | Rate: 2,100 msg/s | Elapsed: 4.8s
...
ğŸ‰ DONE: xxx messages in Xs
```

### âš¡ Option B: TURBO Mode (Nhanh nháº¥t)

```bash
docker exec -it spark-master python3 /opt/src/pipelines/ingestion/producer_turbo.py
```

> âš¡ **TURBO Mode**: Gá»­i data tá»‘c Ä‘á»™ Tá»I ÄA
> - KhÃ´ng giá»¯ timestamp gá»‘c (ghi Ä‘Ã¨ báº±ng thá»i gian hiá»‡n táº¡i)
> - KhÃ´ng delay, khÃ´ng giáº£ láº­p realtime
> - Tá»‘c Ä‘á»™: ~10,000+ msg/s

### ğŸ¢ Option C: Normal Mode (Realtime simulation)

```bash
docker exec -it spark-master python3 /opt/src/pipelines/ingestion/producer.py
```

> ğŸ¢ **Normal Mode**: Giáº£ láº­p thá»i gian thá»±c
> - Giá»¯ timestamp gá»‘c
> - Cháº­m, phÃ¹ há»£p demo realtime
> - Tá»‘c Ä‘á»™: ~5 msg/s (vá»›i x200 speed factor)

---

### ğŸ“Š So sÃ¡nh 3 modes:

| Mode | Tá»‘c Ä‘á»™ | Thá»i gian 1M msg | Giá»¯ timestamp | Use case |
|------|--------|------------------|---------------|----------|
| **producer.py** | ~5 msg/s | ~55 giá» | âœ… | Demo realtime |
| **producer_balanced.py** â­ | ~2,000 msg/s | ~8 phÃºt | âœ… | **Dev/Test** |
| **producer_turbo.py** | ~10,000+ msg/s | ~2 phÃºt | âŒ | Load data nhanh |

**Äá»£i Producer cháº¡y xong** hoáº·c nháº¥n `Ctrl+C` khi Ä‘á»§ data.

---

## BÆ¯á»šC 6: Kiá»ƒm tra Káº¿t quáº£ trong MinIO

```bash
# Kiá»ƒm tra data Ä‘Ã£ Ä‘Æ°á»£c ghi
docker exec minio mc ls local/datalake/raw/music_logs/ --recursive --summarize
```

**Hoáº·c truy cáº­p MinIO Console:**

- URL: http://localhost:9001
- Username: `minioadmin`
- Password: `minioadmin`
- Navigate to: `datalake` â†’ `raw` â†’ `music_logs`

---

## BÆ¯á»šC 7: ETL Master Data (MinIO â†’ MongoDB songs)

**Dá»«ng Spark Streaming** (Terminal 1) báº±ng `Ctrl+C`, sau Ä‘Ã³ restart Spark cluster:

```bash
# Restart Spark Ä‘á»ƒ giáº£i phÃ³ng resources
docker restart spark-master spark-worker

# Äá»£i 15-20 giÃ¢y cho cluster sáºµn sÃ ng
```

**Cháº¡y ETL Master Data:**

```bash
docker exec spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,org.mongodb.spark:mongo-spark-connector_2.12:10.3.0 /opt/src/pipelines/batch/etl_master_data.py
```

**Output mong Ä‘á»£i:**

```
Báº¯t Ä‘áº§u ETL Master Data (Collection: songs)...
>>> Äang Ä‘á»c dá»¯ liá»‡u tá»« MinIO...
>>> Äang lá»c bÃ i hÃ¡t duy nháº¥t...
>>> Äang ghi vÃ o MongoDB...
THÃ€NH CÃ”NG! ÄÃ£ lÆ°u xxx bÃ i hÃ¡t vÃ o MongoDB.
```

---

## BÆ¯á»šC 8: ETL Users (MinIO â†’ MongoDB users)

```bash
docker exec spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,org.mongodb.spark:mongo-spark-connector_2.12:10.3.0 /opt/src/pipelines/batch/etl_users.py
```

**Output mong Ä‘á»£i:**

```
ğŸµ ETL Users Collection (MongoDB)
>>> Äang Ä‘á»c dá»¯ liá»‡u tá»« MinIO...
>>> Äang lá»c users duy nháº¥t...
>>> Äang ghi vÃ o MongoDB...
âœ… THÃ€NH CÃ”NG! ÄÃ£ lÆ°u xxx users vÃ o MongoDB.
```

---

## BÆ¯á»šC 9: Train ALS & Sync Vectors (MongoDB + Milvus)

**Cháº¡y Training:**

```bash
docker exec spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 /opt/src/pipelines/batch/train_als_model.py
```

**Output mong Ä‘á»£i:**

```
============================================================
ğŸµ MUSIC RECOMMENDATION - ALS BATCH TRAINING
   Started at: 2026-01-18 05:00:00
============================================================
>>> Loading data from MinIO (Last 90 days)...
>>> Preparing data for ALS...
>>> Training ALS Model...
    Rank: 64, MaxIter: 15, RegParam: 0.1
>>> ALS Model trained successfully!
>>> Syncing User Factors to MongoDB...
>>> MongoDB: Upserted xxx users
>>> Syncing Item Factors to Milvus...
>>> Setting up Milvus collection 'music_collection'...
>>> Milvus collection created with dimension=64
    Inserted batch 1: 1000 items
    Inserted batch 2: 1000 items
    ...
>>> Milvus: Inserted xxx item embeddings
============================================================
âœ… TRAINING COMPLETED SUCCESSFULLY!
   Users synced to MongoDB: xxx
   Items synced to Milvus: xxx
============================================================
```

---

## BÆ¯á»šC 10: Kiá»ƒm tra Káº¿t quáº£

### MongoDB

```bash
# Äáº¿m sá»‘ bÃ i hÃ¡t
docker exec mongodb mongo music_recsys --eval "db.songs.count()"

# Xem 1 bÃ i hÃ¡t máº«u
docker exec mongodb mongo music_recsys --eval "db.songs.findOne()"

# Äáº¿m sá»‘ users
docker exec mongodb mongo music_recsys --eval "db.users.count()"

# Xem user vá»›i latent_vector (kiá»ƒm tra Ä‘Ã£ cÃ³ vector chÆ°a)
docker exec mongodb mongo music_recsys --quiet --eval "var u = db.users.findOne(); print('User:', u._id); print('Vector length:', u.latent_vector ? u.latent_vector.length : 0)"
```

### Milvus

```bash
# Kiá»ƒm tra Milvus collection
docker exec spark-master python3 -c "
from pymilvus import connections, Collection, utility
connections.connect(host='milvus', port=19530)
print('Collections:', utility.list_collections())
if 'music_collection' in utility.list_collections():
    c = Collection('music_collection')
    print('Entities:', c.num_entities)
    print('Schema:', c.schema)
connections.disconnect('default')
"
```

---

## Monitoring & Debug

### Kafka UI

- URL: http://localhost:8080
- Xem topic `music_log` vÃ  sá»‘ messages

### Spark Master UI

- URL: http://localhost:9090
- Xem running applications vÃ  jobs

### MinIO Console

- URL: http://localhost:9001
- Xem data Ä‘Ã£ Ä‘Æ°á»£c ghi

### Milvus (Check via logs)

```bash
docker logs milvus-standalone --tail 20
```

---

## Dá»«ng Pipeline

```bash
# Terminal cháº¡y Producer: Ctrl+C
# Terminal cháº¡y Spark Streaming: Ctrl+C

# Táº¯t táº¥t cáº£ services
docker-compose down
```

---

## Cháº¡y láº¡i tá»« Ä‘áº§u (Reset)

```bash
# 1. XÃ³a data cÅ© trÃªn mÃ¡y local
rm -rf data/raw data/data_clean data/processed_sorted

# 2. XÃ³a MinIO data (trong container)
docker exec minio mc rm local/datalake/checkpoints/ --recursive --force
docker exec minio mc rm local/datalake/raw/ --recursive --force

# 3. XÃ³a MongoDB data
docker exec mongodb mongo music_recsys --eval "db.songs.drop()"
docker exec mongodb mongo music_recsys --eval "db.users.drop()"

# 4. XÃ³a Milvus data (restart container)
docker-compose restart milvus-standalone

# 5. Cháº¡y láº¡i tá»« BÆ¯á»šC 1
```

---

## Cáº¥u hÃ¬nh quan trá»ng

| Config             | File                                           | GiÃ¡ trá»‹                     |
| ------------------ | ---------------------------------------------- | --------------------------- |
| Kafka Bootstrap    | `producer*.py`, `stream_to_minio*.py`          | `kafka:9092`                |
| MinIO Endpoint     | `stream_to_minio*.py`, `etl_master_data.py`    | `http://minio:9000`         |
| MongoDB URI        | `etl_master_data.py`, `train_als_model.py`     | `mongodb://mongodb:27017`   |
| **Milvus Host**    | `train_als_model.py`                           | `milvus:19530`              |
| Spark Master       | `etl_master_data.py`                           | `spark://spark-master:7077` |
| Processing Trigger | `stream_to_minio.py`                           | `1 minute`                  |
| Processing Trigger | `stream_to_minio_turbo.py` âš¡                  | `10 seconds`                |
| Producer Speed     | `producer.py`                                  | `x200` (realtime simulation)|
| Producer Speed     | `producer_balanced.py` â­                      | `x1000` (fast + timestamp)  |
| Producer Speed     | `producer_turbo.py` âš¡                         | `MAX` (no delay)            |
| **ALS Rank**       | `train_als_model.py`                           | `64` (vector dimension)     |
| **Sliding Window** | `train_als_model.py`                           | `90 days`                   |

---

## Troubleshooting

### 1. Spark job bÃ¡o "Initial job has not accepted any resources"

**NguyÃªn nhÃ¢n:** Spark Worker chÆ°a register hoáº·c Ä‘ang báº­n vá»›i job khÃ¡c.

**Giáº£i phÃ¡p:**

```bash
# Restart Spark cluster
docker restart spark-master spark-worker

# Äá»£i 15-20 giÃ¢y rá»“i cháº¡y láº¡i
```

### 2. MongoDB khÃ´ng khá»Ÿi Ä‘á»™ng Ä‘Æ°á»£c (Exit code 62)

**NguyÃªn nhÃ¢n:** PhiÃªn báº£n MongoDB má»›i yÃªu cáº§u CPU há»— trá»£ AVX.

**Giáº£i phÃ¡p:** Sá»­ dá»¥ng MongoDB 4.4 trong file `.env`:

```
MONGO_IMAGE=mongo:4.4
```

### 3. Docker Desktop khÃ´ng cháº¡y

**NguyÃªn nhÃ¢n:** Docker Desktop chÆ°a start.

**Giáº£i phÃ¡p:**

```bash
# Windows: Má»Ÿ Docker Desktop tá»« Start Menu
# Hoáº·c cháº¡y:
Start-Process "C:\Program Files\Docker\Docker\Docker Desktop.exe"

# Äá»£i Docker ready rá»“i kiá»ƒm tra:
docker info
```

### 4. Producer khÃ´ng gá»­i Ä‘Æ°á»£c message

**NguyÃªn nhÃ¢n:** Kafka chÆ°a healthy.

**Giáº£i phÃ¡p:**

```bash
# Kiá»ƒm tra Kafka status
docker-compose ps kafka

# Äá»£i Ä‘áº¿n khi Kafka healthy rá»“i cháº¡y láº¡i
```

### 5. Milvus khÃ´ng khá»Ÿi Ä‘á»™ng

**NguyÃªn nhÃ¢n:** etcd hoáº·c MinIO chÆ°a sáºµn sÃ ng.

**Giáº£i phÃ¡p:**

```bash
# Kiá»ƒm tra etcd
docker logs milvus-etcd

# Restart Milvus
docker-compose restart milvus-standalone

# Äá»£i 1-2 phÃºt cho Milvus khá»Ÿi Ä‘á»™ng
```

### 6. Lá»—i "Connection refused" khi sync Milvus

**NguyÃªn nhÃ¢n:** Milvus chÆ°a fully started.

**Giáº£i phÃ¡p:**

```bash
# Kiá»ƒm tra Milvus health
docker logs milvus-standalone 2>&1 | grep -i "successfully"

# Äá»£i tháº¥y dÃ²ng "Milvus Proxy successfully initialized" rá»“i cháº¡y láº¡i
```

### 7. Lá»—i "'list' object has no attribute 'toArray'"

**NguyÃªn nhÃ¢n:** Spark 3.5 ALS tráº£ vá» features dáº¡ng list thay vÃ¬ DenseVector.

**Giáº£i phÃ¡p:** ÄÃ£ Ä‘Æ°á»£c fix trong `train_als_model.py` - sá»­ dá»¥ng hÃ m `convert_vector()` Ä‘á»ƒ handle cáº£ 2 trÆ°á»ng há»£p.

---

## ğŸ“ Cáº¥u trÃºc Project

```
music-recsys/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # Data download tá»« HuggingFace
â”‚   â”œâ”€â”€ data_clean/             # Data Ä‘Ã£ clean
â”‚   â””â”€â”€ processed_sorted/       # Data Ä‘Ã£ sort theo timestamp
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ prepare-data/
â”‚   â”‚   â”œâ”€â”€ download_data.py    # Download tá»« HuggingFace
â”‚   â”‚   â”œâ”€â”€ fix_format.py       # Clean data
â”‚   â”‚   â””â”€â”€ etl_sort.py         # Sort theo timestamp
â”‚   â””â”€â”€ pipelines/
â”‚       â”œâ”€â”€ ingestion/
â”‚       â”‚   â”œâ”€â”€ producer.py         # ğŸ¢ Gá»­i data (cháº­m, simulate realtime x200)
â”‚       â”‚   â”œâ”€â”€ producer_balanced.py # â­ CÃ¢n báº±ng speed/realtime (x1000)
â”‚       â”‚   â”œâ”€â”€ producer_turbo.py   # âš¡ Gá»­i data tá»‘c Ä‘á»™ MAX
â”‚       â”‚   â”œâ”€â”€ stream_to_minio.py  # Spark Streaming: Kafka â†’ MinIO (1 min trigger)
â”‚       â”‚   â””â”€â”€ stream_to_minio_turbo.py  # âš¡ Turbo mode (10s trigger)
â”‚       â””â”€â”€ batch/
â”‚           â”œâ”€â”€ etl_master_data.py  # ETL: MinIO â†’ MongoDB (songs)
â”‚           â”œâ”€â”€ etl_users.py        # ETL: MinIO â†’ MongoDB (users)
â”‚           â””â”€â”€ train_als_model.py  # ALS Training â†’ MongoDB + Milvus
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ spark.Dockerfile
â”œâ”€â”€ .env
â””â”€â”€ run-pipeline.md             # File hÆ°á»›ng dáº«n nÃ y
```

---

## ğŸ“Š Káº¿t quáº£ Pipeline

Sau khi cháº¡y xong toÃ n bá»™ pipeline:

| Database    | Collection         | Ná»™i dung                                          |
| :---------- | :----------------- | :------------------------------------------------ |
| **MongoDB** | `songs`            | Metadata bÃ i hÃ¡t (id, title, artist, track_index) |
| **MongoDB** | `users`            | User profile + latent_vector (64-dim)             |
| **Milvus**  | `music_collection` | Item embeddings (64-dim) vá»›i Index IVF_FLAT       |

### Sá»­ dá»¥ng cho Recommendation:

1. **Home Page (User-based):** Query MongoDB `users.latent_vector` â†’ Search Milvus `music_collection` â†’ Top-K songs
2. **Next Song (Item-based):** Láº¥y embedding cá»§a bÃ i Ä‘ang nghe tá»« Milvus â†’ Search similar â†’ Top-K songs

---

## âš¡ Quick Start (Khuyáº¿n nghá»‹)

Náº¿u báº¡n Ä‘Ã£ cÃ³ data trong `data/processed_sorted/`, cháº¡y nhanh:

```bash
# Terminal 1: Streaming
docker exec -it spark-master bash -c "cd /opt/src/pipelines/ingestion && spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 stream_to_minio_turbo.py"

# Terminal 2: Producer BALANCED (má»Ÿ terminal má»›i) - â­ Khuyáº¿n nghá»‹
docker exec -it spark-master python3 /opt/src/pipelines/ingestion/producer_balanced.py

# Hoáº·c dÃ¹ng TURBO náº¿u muá»‘n nhanh nháº¥t (khÃ´ng giá»¯ timestamp gá»‘c):
# docker exec -it spark-master python3 /opt/src/pipelines/ingestion/producer_turbo.py

# Sau khi xong, Ctrl+C cáº£ 2 terminal, restart spark rá»“i cháº¡y:
docker restart spark-master spark-worker

# ETL + Training (chá» 15-20s sau restart)
docker exec spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,org.mongodb.spark:mongo-spark-connector_2.12:10.3.0 /opt/src/pipelines/batch/etl_master_data.py

docker exec spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4,org.mongodb.spark:mongo-spark-connector_2.12:10.3.0 /opt/src/pipelines/batch/etl_users.py

docker exec spark-master spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.hadoop:hadoop-aws:3.3.4 /opt/src/pipelines/batch/train_als_model.py
```

