import pyarrow.parquet as pq
import json
import glob
import os
import shutil

# C·∫•u h√¨nh
INPUT_DIR = "/opt/data/processed_sorted"
OUTPUT_DIR = "/opt/data/songs_master_list"
OUTPUT_FILE = f"{OUTPUT_DIR}/songs.json"
BATCH_SIZE = 50000  # X·ª≠ l√Ω 50.000 d√≤ng m·ªói l·∫ßn (R·∫•t an to√†n cho RAM)

def main():
    print(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω 15 tri·ªáu d√≤ng log (Batch Size: {BATCH_SIZE})...")
    
    # 1. T√¨m file input
    files = glob.glob(f"{INPUT_DIR}/*.parquet")
    if not files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file input.")
        return
    
    # 2. Reset output
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 3. B·ªô nh·ªõ ƒë·ªám Global ƒë·ªÉ kh·ª≠ tr√πng l·∫∑p
    # L∆∞u 1 tri·ªáu ID b√†i h√°t (d·∫°ng chu·ªói) ch·ªâ t·ªën kho·∫£ng 50MB - 100MB RAM -> An to√†n.
    seen_ids = set()
    total_processed = 0
    total_songs_saved = 0

    print("‚è≥ ƒêang ch·∫°y Streaming...")

    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:
        for file_path in files:
            print(f"   üìÇ Reading file: {os.path.basename(file_path)}")
            
            # M·ªü file Parquet ·ªü ch·∫ø ƒë·ªô Stream
            parquet_file = pq.ParquetFile(file_path)
            
            # Duy·ªát qua t·ª´ng nh√≥m d√≤ng (Batch)
            for batch in parquet_file.iter_batches(batch_size=BATCH_SIZE, columns=[
                "musicbrainz_track_id", "track_name", 
                "musicbrainz_artist_id", "artist_name"
            ]):
                # Chuy·ªÉn Batch sang Pandas DataFrame (Ch·ªâ t·ªën RAM cho 50k d√≤ng)
                df = batch.to_pandas()
                
                # ƒê·ªïi t√™n c·ªôt
                df = df.rename(columns={
                    "musicbrainz_track_id": "_id",
                    "track_name": "title",
                    "musicbrainz_artist_id": "artist_id",
                    "artist_name": "artist"
                })

                # L·ªçc r√°c
                df = df.dropna(subset=["_id", "title"])
                
                # X·ª≠ l√Ω ghi file
                batch_json_lines = []
                for _, row in df.iterrows():
                    # Check tr√πng l·∫∑p c·ª±c nhanh b·∫±ng Set
                    if row['_id'] not in seen_ids:
                        seen_ids.add(row['_id'])
                        batch_json_lines.append(json.dumps(row.to_dict(), ensure_ascii=False))
                
                # Ghi xu·ªëng ƒëƒ©a ngay l·∫≠p t·ª©c
                if batch_json_lines:
                    f_out.write("\n".join(batch_json_lines) + "\n")
                    total_songs_saved += len(batch_json_lines)

                # C·∫≠p nh·∫≠t ti·∫øn ƒë·ªô
                total_processed += len(df)
                if total_processed % 500000 == 0:
                    print(f"      -> ƒê√£ qu√©t {total_processed:,} d√≤ng... (L·∫•y ƒë∆∞·ª£c {total_songs_saved:,} b√†i)")

    print("‚úÖ HO√ÄN T·∫§T!")
    print(f"   - T·ªïng log ƒë√£ qu√©t: {total_processed:,}")
    print(f"   - T·ªïng b√†i h√°t s·∫°ch: {total_songs_saved:,}")
    print(f"   - Output: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()