"""
ETL Users Collection - MongoDB
==============================
Tr√≠ch xu·∫•t danh s√°ch users duy nh·∫•t t·ª´ logs v√† t·∫°o s·∫µn
trong MongoDB ƒë·ªÉ chu·∫©n b·ªã cho vi·ªác sync latent_vector sau n√†y.
"""

import sys
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, first, current_timestamp
from pyspark.sql.types import ArrayType, FloatType

# Import config t·∫≠p trung
import src.config as cfg

def run_users_etl():
    print("=" * 60)
    print("üéµ ETL Users Collection (MongoDB)")
    print(f"   Started at: {datetime.now()}")
    print("=" * 60)
    
    # 1. Kh·ªüi t·∫°o Spark (Kh√¥ng c·∫ßn spark.jars.packages - Docker ƒë√£ t√≠ch h·ª£p s·∫µn)
    spark = SparkSession.builder \
        .appName("ETL_Users_Master") \
        .master("spark://spark-master:7077") \
        .config("spark.executor.memory", "1g") \
        .config("spark.executor.cores", "1") \
        .config("spark.cores.max", "1") \
        .config("spark.hadoop.fs.s3a.endpoint", cfg.MINIO_ENDPOINT) \
        .config("spark.hadoop.fs.s3a.access.key", cfg.MINIO_ACCESS_KEY) \
        .config("spark.hadoop.fs.s3a.secret.key", cfg.MINIO_SECRET_KEY) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.mongodb.write.connection.uri", f"{cfg.MONGO_URI}/{cfg.MONGO_DB}.{cfg.COLLECTION_USERS}") \
        .getOrCreate()

    # 2. ƒê·ªçc d·ªØ li·ªáu t·ª´ MinIO
    print(">>> ƒêang ƒë·ªçc d·ªØ li·ªáu t·ª´ MinIO...")
    try:
        df = spark.read.parquet(cfg.MINIO_RAW_MUSIC_LOGS_PATH)
    except Exception as e:
        print(f"L·ªói ƒë·ªçc MinIO (C√≥ th·ªÉ do ch∆∞a c√≥ data): {e}")
        spark.stop()
        return

    # 3. Tr√≠ch xu·∫•t danh s√°ch users duy nh·∫•t
    print(">>> ƒêang l·ªçc users duy nh·∫•t...")
    users_unique = df.select("user_id").distinct() \
        .withColumn("username", col("user_id")) \
        .withColumn("latent_vector", lit(None).cast(ArrayType(FloatType()))) \
        .withColumn("last_updated", lit(None).cast("timestamp")) \
        .select(
            col("user_id").alias("_id"),
            col("username"),
            col("latent_vector"),
            col("last_updated")
        )

    # 4. Ghi v√†o MongoDB (Mode: Append - Kh√¥ng ghi ƒë√® users ƒë√£ c√≥ vector)
    # S·ª≠ d·ª•ng upsert ƒë·ªÉ ch·ªâ th√™m users m·ªõi, kh√¥ng x√≥a latent_vector ƒë√£ c√≥
    print(">>> ƒêang ghi v√†o MongoDB...")
    
    # ƒê·∫øm tr∆∞·ªõc khi ghi
    user_count = users_unique.count()
    
    # Ghi v·ªõi mode overwrite (s·∫Ω ƒë∆∞·ª£c thay b·∫±ng upsert logic trong train_als_model.py)
    users_unique.write \
        .format("mongodb") \
        .mode("overwrite") \
        .option("database", cfg.MONGO_DB) \
        .option("collection", cfg.COLLECTION_USERS) \
        .save()

    print(f"TH√ÄNH C√îNG! ƒê√£ l∆∞u {user_count} users v√†o MongoDB.")
    print(f"   Completed at: {datetime.now()}")
    print("=" * 60)
    spark.stop()

if __name__ == "__main__":
    run_users_etl()
