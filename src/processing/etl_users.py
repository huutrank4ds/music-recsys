"""
ETL Users Collection - MongoDB
==============================
TrÃ­ch xuáº¥t danh sÃ¡ch users duy nháº¥t tá»« logs vÃ  táº¡o sáºµn
trong MongoDB Ä‘á»ƒ chuáº©n bá»‹ cho viá»‡c sync latent_vector sau nÃ y.
"""

import sys
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, first, current_timestamp
from pyspark.sql.types import ArrayType, FloatType

# Cáº¥u hÃ¬nh Packages
PACKAGES = [
    "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0",
    "org.apache.hadoop:hadoop-aws:3.3.4",
    "org.mongodb.spark:mongo-spark-connector_2.12:10.3.0"
]

def run_users_etl():
    print("=" * 60)
    print("ğŸµ ETL Users Collection (MongoDB)")
    print(f"   Started at: {datetime.now()}")
    print("=" * 60)
    
    # 1. Khá»Ÿi táº¡o Spark
    spark = SparkSession.builder \
        .appName("ETL_Users_Master") \
        .master("spark://spark-master:7077") \
        .config("spark.jars.packages", ",".join(PACKAGES)) \
        .config("spark.executor.memory", "1g") \
        .config("spark.executor.cores", "1") \
        .config("spark.cores.max", "1") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.secret.key", "minioadmin") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.mongodb.write.connection.uri", "mongodb://mongodb:27017/music_recsys.users") \
        .getOrCreate()

    # 2. Äá»c dá»¯ liá»‡u tá»« MinIO
    print(">>> Äang Ä‘á»c dá»¯ liá»‡u tá»« MinIO...")
    try:
        df = spark.read.parquet("s3a://datalake/raw/music_logs/")
    except Exception as e:
        print(f"Lá»—i Ä‘á»c MinIO (CÃ³ thá»ƒ do chÆ°a cÃ³ data): {e}")
        spark.stop()
        return

    # 3. TrÃ­ch xuáº¥t danh sÃ¡ch users duy nháº¥t
    print(">>> Äang lá»c users duy nháº¥t...")
    users_unique = df.select("user_id").distinct() \
        .withColumn("username", col("user_id")) \
        .withColumn("latent_vector", lit(None).cast(ArrayType(FloatType()))) \
        .withColumn("last_updated", lit(None).cast("timestamp")) \
        .select(
            col("user_id").alias("_id"),
            col("username"),
            col("latent_vector"),
            col("last_updated")
        )

    # 4. Ghi vÃ o MongoDB (Mode: Append - KhÃ´ng ghi Ä‘Ã¨ users Ä‘Ã£ cÃ³ vector)
    # Sá»­ dá»¥ng upsert Ä‘á»ƒ chá»‰ thÃªm users má»›i, khÃ´ng xÃ³a latent_vector Ä‘Ã£ cÃ³
    print(">>> Äang ghi vÃ o MongoDB...")
    
    # Äáº¿m trÆ°á»›c khi ghi
    user_count = users_unique.count()
    
    # Ghi vá»›i mode overwrite (sáº½ Ä‘Æ°á»£c thay báº±ng upsert logic trong train_als_model.py)
    users_unique.write \
        .format("mongodb") \
        .mode("overwrite") \
        .option("database", "music_recsys") \
        .option("collection", "users") \
        .save()

    print(f"THÃ€NH CÃ”NG! ÄÃ£ lÆ°u {user_count} users vÃ o MongoDB.")
    print(f"   Completed at: {datetime.now()}")
    print("=" * 60)
    spark.stop()

if __name__ == "__main__":
    run_users_etl()
