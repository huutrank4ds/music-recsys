import pyarrow as pa
import pyarrow.parquet as pq
from pathlib import Path
import shutil

SOURCE_DIR = Path('/opt/data/')
DEST_DIR = Path('/opt/data_clean/')

def rebirth_parquet():
    print("üî• B·∫ÆT ƒê·∫¶U QUY TR√åNH T√ÅI SINH D·ªÆ LI·ªÜU (PANDAS CHUNKING)")
    
    if DEST_DIR.exists(): shutil.rmtree(DEST_DIR)
    DEST_DIR.mkdir(parents=True, exist_ok=True)

    files = [f for f in SOURCE_DIR.glob("*.parquet") if f.is_file() and not f.name.startswith("_")]

    for file_path in files:
        print(f"üëâ X·ª≠ l√Ω: {file_path.name}")
        dest_path = DEST_DIR / file_path.name
        
        try:
            # 1. M·ªü file Parquet b·∫±ng PyArrow ƒë·ªÉ l·∫•y Schema tr∆∞·ªõc
            pf = pq.ParquetFile(file_path)
            
            # T·∫°o Writer m·ªõi
            first_chunk = True
            
            # 2. ƒê·ªçc t·ª´ng chunk b·∫±ng Pandas (Batch size an to√†n)
            for df_chunk in pf.iter_batches(batch_size=50000):
                df = df_chunk.to_pandas()
                
                # --- CHUY·ªÇN ƒê·ªîI TIMESTAMP M·∫†NH TAY ---
                if 'timestamp' in df.columns:
                    # √âp v·ªÅ datetime64[us] (Microseconds)
                    df['timestamp'] = df['timestamp'].astype('datetime64[us]')

                # 3. Ghi ra file m·ªõi
                # C√°ch t·ªët nh·∫•t v·ªõi PyArrow Table ƒë·ªÉ ghi append
                table = pa.Table.from_pandas(df)
                
                if first_chunk:
                    writer = pq.ParquetWriter(dest_path, table.schema, coerce_timestamps='us', allow_truncated_timestamps=True)
                    first_chunk = False
                
                writer.write_table(table)
            
            if not first_chunk:
                writer.close()
                print(f"   ‚úÖ ƒê√£ t√°i sinh th√†nh c√¥ng: {dest_path}")

        except Exception as e:
            print(f"   ‚ùå L·ªói: {e}")

if __name__ == "__main__":
    rebirth_parquet()