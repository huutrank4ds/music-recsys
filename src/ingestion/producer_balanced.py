"""
BALANCED PRODUCER - C√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô v√† t√≠nh realtime
=========================================================
- Gi·ªØ th·ª© t·ª± timestamp (quan tr·ªçng cho time-series)
- G·ª≠i theo micro-batch (nhanh h∆°n t·ª´ng event)
- Nh·∫£y qua kho·∫£ng tr·ªëng l·ªõn (skip idle time)
- T·ªëc ƒë·ªô: ~1000-5000 msg/s (nhanh h∆°n Normal 1000x, ch·∫≠m h∆°n TURBO 2-10x)
"""
import json
import socket
import time
from datetime import datetime
import pyarrow.parquet as pq
from pathlib import Path
from confluent_kafka import Producer
from confluent_kafka.admin import AdminClient, NewTopic

# ================= C·∫§U H√åNH =================
# T·ªëc ƒë·ªô tƒÉng t·ªëc (x1000 = 1 gi·ªù data ch·∫°y trong 3.6s)
SPEED_FACTOR = 1000.0

# Micro-batch: G·ª≠i bao nhi√™u events tr∆∞·ªõc khi check time
MICRO_BATCH_SIZE = 500

# Th·ªùi gian t·ªëi ƒëa ch·ªù gi·ªØa c√°c batch (gi√¢y th·ª±c)
MAX_WAIT_BETWEEN_BATCHES = 0.5

# Nh·∫£y qua kho·∫£ng tr·ªëng l·ªõn h∆°n X ph√∫t (trong data)
SKIP_GAP_MINUTES = 5

# ================= C·∫§U H√åNH KAFKA (T·ªêI ∆ØU) =================
CONF = {
    'bootstrap.servers': 'kafka:9092',
    'client.id': socket.gethostname(),
    'acks': '1',
    'linger.ms': 20,
    'batch.size': 65536,  # 64KB
    'compression.type': 'lz4',
    'queue.buffering.max.messages': 50000,
}

TOPIC = "music_log"
DATA_DIR = Path('/opt/data/processed_sorted')
TIMESTAMP_COL = 'timestamp'
NUM_PARTITIONS = 4
REPLICATION_FACTOR = 1

# ================= H√ÄM QU·∫¢N L√ù TOPIC =================
def ensure_topic_exists():
    print(f"üîß ƒêang ki·ªÉm tra Topic '{TOPIC}'...")
    admin_client = AdminClient({'bootstrap.servers': CONF['bootstrap.servers']})
    cluster_metadata = admin_client.list_topics(timeout=10)
    
    if TOPIC in cluster_metadata.topics:
        print(f"‚úÖ Topic '{TOPIC}' ƒë√£ t·ªìn t·∫°i.")
    else:
        print(f"‚ö†Ô∏è ƒêang t·∫°o topic v·ªõi {NUM_PARTITIONS} partitions...")
        new_topic = NewTopic(topic=TOPIC, num_partitions=NUM_PARTITIONS, replication_factor=REPLICATION_FACTOR)
        fs = admin_client.create_topics([new_topic])
        for topic, future in fs.items():
            try:
                future.result()
                print(f"üéâ ƒê√£ t·∫°o topic: {topic}")
            except Exception as e:
                print(f"‚ùå L·ªói t·∫°o topic: {e}")

# ================= BALANCED GENERATOR =================
def balanced_data_generator(data_dir):
    """
    Generator v·ªõi c∆° ch·∫ø:
    1. ƒê·ªçc theo micro-batch (kh√¥ng ph·∫£i t·ª´ng event)
    2. Gi·ªØ th·ª© t·ª± timestamp
    3. TƒÉng t·ªëc theo SPEED_FACTOR
    4. Nh·∫£y qua kho·∫£ng tr·ªëng l·ªõn
    """
    files = sorted([f for f in data_dir.glob("*.parquet") if f.is_file() and not f.name.startswith('.')])
    if not files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file.")
        return

    print(f"‚ö° BALANCED MODE: x{SPEED_FACTOR} speed, batch {MICRO_BATCH_SIZE}")
    print(f"   Skip gaps > {SKIP_GAP_MINUTES} minutes")
    
    first_data_ts = None
    wall_clock_start = None
    total_skipped_time = 0
    last_data_ts = None
    
    for file_path in files:
        print(f"\nüìñ ƒê·ªçc file: {file_path.name}")
        parquet_file = pq.ParquetFile(file_path)

        for batch in parquet_file.iter_batches(batch_size=MICRO_BATCH_SIZE):
            records = batch.to_pylist()
            batch_records = []
            
            for record in records:
                original_ts_str = record.get(TIMESTAMP_COL)
                if not original_ts_str:
                    continue
                    
                try:
                    if isinstance(original_ts_str, str):
                        current_data_ts = datetime.fromisoformat(original_ts_str)
                    else:
                        current_data_ts = original_ts_str
                except ValueError:
                    continue

                # Kh·ªüi t·∫°o l·∫ßn ƒë·∫ßu
                if first_data_ts is None:
                    first_data_ts = current_data_ts
                    wall_clock_start = time.time()
                    last_data_ts = current_data_ts

                # Ki·ªÉm tra c√≥ gap l·ªõn kh√¥ng
                if last_data_ts:
                    gap_seconds = (current_data_ts - last_data_ts).total_seconds()
                    if gap_seconds > SKIP_GAP_MINUTES * 60:
                        skip_amount = gap_seconds - 1  # Gi·ªØ l·∫°i 1 gi√¢y
                        total_skipped_time += skip_amount
                        print(f"‚è© Skip gap {gap_seconds/60:.1f} ph√∫t")

                last_data_ts = current_data_ts
                
                # QUAN TR·ªåNG: Gi·ªØ timestamp g·ªëc (kh√¥ng ghi ƒë√®)
                batch_records.append(record)

            # Yield to√†n b·ªô batch
            if batch_records:
                # T√≠nh th·ªùi gian c·∫ßn ch·ªù cho batch n√†y
                if first_data_ts and wall_clock_start:
                    batch_end_ts = last_data_ts
                    elapsed_data_seconds = (batch_end_ts - first_data_ts).total_seconds() - total_skipped_time
                    target_wall_time = wall_clock_start + (elapsed_data_seconds / SPEED_FACTOR)
                    sleep_time = target_wall_time - time.time()
                    
                    if 0 < sleep_time <= MAX_WAIT_BETWEEN_BATCHES:
                        time.sleep(sleep_time)
                    elif sleep_time > MAX_WAIT_BETWEEN_BATCHES:
                        # N·∫øu c·∫ßn ch·ªù qu√° l√¢u, ch·ªâ ch·ªù max
                        time.sleep(MAX_WAIT_BETWEEN_BATCHES)
                
                yield batch_records

# ================= MAIN =================
def delivery_report(err, msg):
    if err is not None:
        print(f'‚ùå L·ªói: {err}')

def run_producer():
    start_time = time.time()
    
    ensure_topic_exists()
    
    print("üîå Kh·ªüi t·∫°o Producer BALANCED...")
    producer = Producer(CONF)
    
    total_sent = 0
    batch_count = 0

    try:
        for batch_records in balanced_data_generator(DATA_DIR):
            # G·ª≠i to√†n b·ªô batch nhanh ch√≥ng
            for record in batch_records:
                msg_value = json.dumps(record, default=str).encode('utf-8')
                producer.produce(TOPIC, value=msg_value, callback=delivery_report)
                total_sent += 1
            
            # Poll ƒë·ªÉ x·ª≠ l√Ω callbacks
            producer.poll(0)
            batch_count += 1
            
            # Log m·ªói 10 batch
            if batch_count % 10 == 0:
                elapsed = time.time() - start_time
                rate = total_sent / elapsed if elapsed > 0 else 0
                print(f"üìä Sent: {total_sent:,} | Rate: {rate:,.0f} msg/s | Elapsed: {elapsed:.1f}s")
        
        producer.flush(30)
        elapsed = time.time() - start_time
        rate = total_sent / elapsed if elapsed > 0 else 0
        print(f"\nüéâ DONE: {total_sent:,} messages in {elapsed:.1f}s ({rate:,.0f} msg/s)")
        
    except KeyboardInterrupt:
        print("\nüõë Stopped.")
    except Exception as e:
        print(f"\nüí• Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    run_producer()
