import json
import socket
import time
import sys
from datetime import datetime
import pyarrow.parquet as pq
from pathlib import Path
# Th√™m AdminClient v√† NewTopic
from confluent_kafka import Producer
from confluent_kafka.admin import AdminClient, NewTopic 

# ================= C·∫§U H√åNH T·ªêC ƒê·ªò =================
SPEED_FACTOR = 200.0  # T·ªëc ƒë·ªô nhanh h∆°n th·ªùi gian th·ª±c g·∫•p bao nhi√™u l·∫ßn
MAX_SLEEP_SEC = 2.0   

# ================= C·∫§U H√åNH KAFKA =================
CONF = {
    'bootstrap.servers': 'kafka:9092',
    'client.id': socket.gethostname(),
    'acks': '1',
    'linger.ms': 5,
    'batch.size': 16384,
    'compression.type': 'gzip',
}

TOPIC = "music_log"
DATA_DIR = Path('/opt/data/processed_sorted')
TIMESTAMP_COL = 'timestamp'
NUM_PARTITIONS = 4
REPLICATION_FACTOR = 1

BATCH_SIZE = 2000  # S·ªë b·∫£n ghi m·ªói l√¥

# ================= H√ÄM QU·∫¢N L√ù TOPIC =================
def ensure_topic_exists():
    """Ki·ªÉm tra v√† t·∫°o Topic n·∫øu ch∆∞a c√≥"""
    print(f"üîß ƒêang ki·ªÉm tra Topic '{TOPIC}'...")
    
    # T·∫°o AdminClient (d√πng chung config v·ªõi Producer)
    admin_client = AdminClient({'bootstrap.servers': CONF['bootstrap.servers']})
    
    # L·∫•y danh s√°ch topic hi·ªán c√≥
    cluster_metadata = admin_client.list_topics(timeout=10)
    
    if TOPIC in cluster_metadata.topics:
        print(f"‚úÖ Topic '{TOPIC}' ƒë√£ t·ªìn t·∫°i.")
    else:
        print(f"‚ö†Ô∏è Topic ch∆∞a c√≥. ƒêang t·∫°o m·ªõi v·ªõi {NUM_PARTITIONS} partitions...")
        # ƒê·ªãnh nghƒ©a topic m·ªõi
        new_topic = NewTopic(
            topic=TOPIC, 
            num_partitions=NUM_PARTITIONS, 
            replication_factor=REPLICATION_FACTOR
        )
        # G·ª≠i l·ªánh t·∫°o
        fs = admin_client.create_topics([new_topic])
        
        # Ch·ªù k·∫øt qu·∫£
        for topic, future in fs.items():
            try:
                future.result()  # Block ch·ªù t·∫°o xong
                print(f"üéâ ƒê√£ t·∫°o th√†nh c√¥ng topic: {topic}")
            except Exception as e:
                print(f"‚ùå Kh√¥ng th·ªÉ t·∫°o topic {topic}: {e}")

# ================= GENERATOR =================
def source_data_generator(data_dir, skip_time=True):
    files = sorted([f for f in data_dir.glob("*.parquet") if f.is_file() and not f.name.startswith('.')])
    if not files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file.")
        return

    first_data_ts = None # Th·ªùi gian d·ªØ li·ªáu ƒë·∫ßu ti√™n
    wall_clock_start = None # Th·ªùi gian th·ª±c khi b·∫Øt ƒë·∫ßu
    time_skip_accumulation = 0 # T·ªïng th·ªùi gian nh·∫£y c√≥c

    print(f"üöÄ B·∫Øt ƒë·∫ßu Replay v·ªõi t·ªëc ƒë·ªô: x{SPEED_FACTOR}")
    
    for file_path in files:
        print(f"\nüìñ ƒê·ªçc file: {file_path.name}")
        parquet_file = pq.ParquetFile(file_path)

        for batch in parquet_file.iter_batches(batch_size=BATCH_SIZE):
            records = batch.to_pylist()
            for record in records:
                original_ts_str = record.get(TIMESTAMP_COL)
                if not original_ts_str: continue
                try:
                    if isinstance(original_ts_str, str):
                        current_data_ts = datetime.fromisoformat(original_ts_str)
                    else:
                        current_data_ts = original_ts_str
                except ValueError: continue

                if first_data_ts is None:
                    first_data_ts = current_data_ts
                    wall_clock_start = time.time()

                elapsed_seconds_ts = (current_data_ts - first_data_ts).total_seconds()
                real_elapsed_ts = elapsed_seconds_ts / SPEED_FACTOR
                real_elapsed_ts -= time_skip_accumulation
                target_wall_time = wall_clock_start + real_elapsed_ts #type: ignore
                sleep_duration = target_wall_time - time.time()

                if sleep_duration > 0:
                    if sleep_duration > MAX_SLEEP_SEC and skip_time:
                        skip_amount = sleep_duration - MAX_SLEEP_SEC
                        time_skip_accumulation += skip_amount
                        print(f"‚è© Nh·∫£y c√≥c {skip_amount:.1f}s...")
                        time.sleep(MAX_SLEEP_SEC)
                    else:
                        time.sleep(sleep_duration)

                record[TIMESTAMP_COL] = datetime.now().isoformat()
                yield record

# ================= MAIN =================
def delivery_report(err, msg):
    if err is not None: print(f'‚ùå L·ªói: {err}')

def run_producer():
    # 1. KI·ªÇM TRA TOPIC TR∆Ø·ªöC KHI CH·∫†Y
    ensure_topic_exists()
    
    # 2. KH·ªûI T·∫†O PRODUCER
    print("üîå Kh·ªüi t·∫°o Producer...")
    producer = Producer(CONF)
    
    data_stream = source_data_generator(DATA_DIR, skip_time=False)
    total_sent = 0

    try:
        for record in data_stream:
            msg_value = json.dumps(record, default=str).encode('utf-8')
            producer.produce(TOPIC, value=msg_value, callback=delivery_report)
            producer.poll(0)
            total_sent += 1
            if total_sent % 100 == 0:
                print(f"‚úÖ Sent: {total_sent} | Time: {record[TIMESTAMP_COL]}", end='\r')
        
        producer.flush(10)
        print(f"\nüéâ DONE: {total_sent}")
    except KeyboardInterrupt:
        print("\nüõë Stopped.")
    except Exception as e:
        print(f"\nüí• Error: {e}")

if __name__ == "__main__":
    run_producer()