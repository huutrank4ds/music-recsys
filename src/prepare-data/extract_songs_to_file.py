import os
import glob
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
# ‚ö†Ô∏è THAY ƒê·ªîI QUAN TR·ªåNG: Th√™m LongType
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType 

# ================= 1. H√ÄM TI·ªÜN √çCH =================
def get_valid_parquet_files(data_dir):
    print(f"üîç ƒêang qu√©t file trong: {data_dir}")
    if not os.path.exists(data_dir):
        print(f"‚ùå Th∆∞ m·ª•c kh√¥ng t·ªìn t·∫°i: {data_dir}")
        return []

    all_files = glob.glob(os.path.join(data_dir, "*.parquet"))
    valid_files = []
    for f in all_files:
        filename = os.path.basename(f)
        if filename.startswith('.') or filename.startswith('_'):
            continue
        valid_files.append(f"file://{f}")
    
    valid_files.sort()
    return valid_files

# ================= 2. H√ÄM CH√çNH =================
def main():
    BASE_DIR = "/opt/data/processed_sorted"
    OUTPUT_DIR = "file:///opt/data/songs_master_list"

    input_files = get_valid_parquet_files(BASE_DIR)
    if not input_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file!")
        return

    print(f"‚úÖ T√¨m th·∫•y {len(input_files)} file s·∫°ch.")

    print("\nüöÄ Kh·ªüi t·∫°o Spark Session...")
    spark = SparkSession.builder \
        .appName("ExtractSongsFixedType") \
        .config("spark.driver.memory", "2g") \
        .getOrCreate()

    # ‚ö†Ô∏è S·ª¨A L·ªñI T·∫†I ƒê√ÇY: D√πng LongType cho c√°c tr∆∞·ªùng Index
    song_schema = StructType([
        StructField("musicbrainz_track_id", StringType(), True),
        StructField("track_name", StringType(), True),
        StructField("musicbrainz_artist_id", StringType(), True),
        StructField("artist_name", StringType(), True),
        StructField("track_index", LongType(), True),   # <--- ƒê√£ s·ª≠a th√†nh LongType
        StructField("artist_index", LongType(), True)   # <--- ƒê√£ s·ª≠a th√†nh LongType
    ])

    try:
        print("üìñ ƒêang ƒë·ªçc d·ªØ li·ªáu...")
        raw_df = spark.read.schema(song_schema).parquet(*input_files)
        
        print("üîÑ ƒêang x·ª≠ l√Ω ETL...")
        songs_df = raw_df.select(
            col("musicbrainz_track_id").alias("id"),
            col("track_name"),
            col("musicbrainz_artist_id"),
            col("artist_name"),
            col("track_index"),
            col("artist_index")
        ).dropDuplicates(["id"])

        count = songs_df.count()
        print(f"üéµ T√¨m th·∫•y t·ªïng c·ªông: {count} b√†i h√°t duy nh·∫•t.")

        print(f"üíæ ƒêang ghi file JSON v√†o: {OUTPUT_DIR}")
        
        # Ghi ƒë√® (overwrite) ƒë·ªÉ x√≥a d·ªØ li·ªáu l·ªói c≈© n·∫øu c√≥
        songs_df.write \
            .mode("overwrite") \
            .json(OUTPUT_DIR)

        print("‚úÖ TH√ÄNH C√îNG! B√¢y gi·ªù b·∫°n h√£y ki·ªÉm tra th∆∞ m·ª•c data.")

    except Exception as e:
        print(f"üí• V·∫™N C√íN L·ªñI: {e}")
    finally:
        spark.stop()

if __name__ == "__main__":
    main()