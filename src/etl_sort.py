from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import os

# --- Cáº¤U HÃŒNH ---
# Äá»c tá»« thÆ° má»¥c Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch (QUAN TRá»ŒNG)
INPUT_PATH = "/opt/data_clean/*.parquet" 
# Ghi ra thÆ° má»¥c káº¿t quáº£
OUTPUT_PATH = "/opt/data/processed_sorted"

def process_data():
    # 1. Khá»Ÿi táº¡o Spark
    print("ğŸš€ Khá»Ÿi Ä‘á»™ng Spark Session...")
    spark = SparkSession.builder \
        .appName("SortLastFM") \
        .config("spark.driver.memory", "2g") \
        .getOrCreate()
    
    # Giáº£m bá»›t log rÃ¡c Ä‘á»ƒ dá»… nhÃ¬n lá»—i náº¿u cÃ³
    spark.sparkContext.setLogLevel("ERROR")

    print(f"â³ Äang Ä‘á»c dá»¯ liá»‡u sáº¡ch tá»«: {INPUT_PATH}")
    
    try:
        # 2. Äá»c dá»¯ liá»‡u
        df = spark.read.parquet(INPUT_PATH)
        
        # In ra schema Ä‘á»ƒ kiá»ƒm tra cháº¯c cháº¯n timestamp lÃ  timestamp (khÃ´ng pháº£i long/int)
        print("ğŸ“Š Schema dá»¯ liá»‡u Ä‘áº§u vÃ o:")
        df.printSchema()
        
        # Äáº¿m sÆ¡ bá»™
        count = df.count()
        print(f"   -> Tá»•ng sá»‘ dÃ²ng: {count:,}")

        # 3. Sáº¯p xáº¿p toÃ n cá»¥c (Global Sort)
        print("â³ Äang sáº¯p xáº¿p theo thá»i gian (timestamp ASC)...")
        sorted_df = df.orderBy(col("timestamp").asc())

        # 4. Ghi ra Ä‘Ä©a
        print(f"ğŸ’¾ Äang ghi dá»¯ liá»‡u Ä‘Ã£ sáº¯p xáº¿p ra: {OUTPUT_PATH}")
        
        # coalesce(1): Gom láº¡i thÃ nh 1 file duy nháº¥t Ä‘á»ƒ Producer Ä‘á»c cho dá»…
        sorted_df.coalesce(1).write.mode("overwrite").parquet(OUTPUT_PATH)
        
        print("âœ… THÃ€NH CÃ”NG! Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ báº¯n vÃ o Kafka.")
        
    except Exception as e:
        print("âŒ Lá»–I Rá»’I:")
        print(e)
        
    finally:
        spark.stop()

if __name__ == "__main__":
    process_data()